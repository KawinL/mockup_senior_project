{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load & train word2vec model\n",
    "model = gensim.models.Word2Vec.load('/Users/AUM/Documents/PROJECT/mockup_senior_project/thai_text_mock_up/word2vec/w2v.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for word in model.wv.vocab:\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 74664\n"
     ]
    }
   ],
   "source": [
    "# Printing out number of tokens available\n",
    "print(\"Number of Tokens: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 0-0000-0000, Similarity: 0.81\n",
      "Word: 00-000-0000, Similarity: 0.75\n",
      "Word: 00000, Similarity: 0.67\n",
      "Word: 0 0000 0000, Similarity: 0.67\n",
      "Word: โทร., Similarity: 0.66\n",
      "Word: N00, Similarity: 0.63\n",
      "Word: อีเมล์, Similarity: 0.62\n",
      "Word: 0-0000-0000-00, Similarity: 0.62\n",
      "Word: 000-0000000, Similarity: 0.60\n",
      "Word: เวสเทิร์นยูเนี่ยน, Similarity: 0.60\n"
     ]
    }
   ],
   "source": [
    "# Pick a word \n",
    "find_similar_to = '00-0000-0000'\n",
    "\n",
    "# Finding out similar words [default= top 10]\n",
    "for similar_word in model.similar_by_word(find_similar_to):\n",
    "    print(\"Word: {0}, Similarity: {1:.2f}\".format(\n",
    "        similar_word[0], similar_word[1]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : จุฬาราชมนตรี , Similarity: 0.67\n",
      "Word : ปิยะสกล , Similarity: 0.65\n",
      "Word : เมฆสวรรค์ , Similarity: 0.64\n",
      "Word : มิลล์วีนา , Similarity: 0.64\n",
      "Word : เลขานุการ , Similarity: 0.64\n",
      "Word : พระครู , Similarity: 0.63\n",
      "Word : หงษ์เหิน , Similarity: 0.63\n",
      "Word : ม.ร.ว. , Similarity: 0.63\n",
      "Word : ปวีณา , Similarity: 0.63\n",
      "Word : ติณสูลานนท์ , Similarity: 0.63\n"
     ]
    }
   ],
   "source": [
    "# Test words \n",
    "word_add = ['นายกรัฐมนตรี', 'นาง']\n",
    "word_sub = ['นาย']\n",
    "\n",
    "# Word vector addition and subtraction \n",
    "for resultant_word in model.most_similar(\n",
    "    positive=word_add, negative=word_sub\n",
    "):\n",
    "    print(\"Word : {0} , Similarity: {1:.2f}\".format(\n",
    "        resultant_word[0], resultant_word[1]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit number of tokens to be visualized\n",
    "limit = 500\n",
    "vector_dim = 100\n",
    "# Getting tokens and vectors\n",
    "words = []\n",
    "embedding = np.array([])\n",
    "i = 0\n",
    "for word in model.wv.vocab:\n",
    "    # Break the loop if limit exceeds \n",
    "    if i == limit: break\n",
    "\n",
    "    # Getting token \n",
    "    words.append(word)\n",
    "\n",
    "    # Appending the vectors \n",
    "    embedding = np.append(embedding, model[word])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# Reshaping the embedding vector \n",
    "embedding = embedding.reshape(limit, vector_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ThaiFont = FontProperties(fname = '/Users/AUM/Library/Fonts/THSarabunChula-Regular.ttf')\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom',\n",
    "                 fontproperties = ThaiFont)\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    \n",
    "# Creating the tsne plot [Warning: will take time]\n",
    "tsne = TSNE(perplexity=30.0, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "low_dim_embedding = tsne.fit_transform(embedding)\n",
    "\n",
    "# Finally plotting and saving the fig \n",
    "plot_with_labels(low_dim_embedding, words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
